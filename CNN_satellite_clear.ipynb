{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Satellite Image Lan Use Classes for Urban Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview: using pre-labeled satellite image data from the EuroSAT database (https://www.tensorflow.org/datasets/catalog/eurosat):\n",
    "\n",
    "Business Case: Urban planning consultants need to filter data for a given city by it's land use. Run an image classification model on pre-trained data, and use domain knowledge to apply the model to unclassified satellite imagery.\n",
    "\n",
    "\n",
    "1. Create a NN model that can accurately predict land cover using the classified dataset\n",
    "1. determine best model architecture and tune hyper-parameters to acheive best accuracy scores\n",
    "1. Predict type of land use on future unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing few libraries\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay, recall_score, f1_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "#Neural network packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D # creates densely connected layer object\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for reference: tutorial followed on loading image data here: https://www.tensorflow.org/tutorials/images/classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing the Images Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "DATASET = \"2750\"\n",
    "LABELS = os.listdir(DATASET)\n",
    "print(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot class distributions of whole dataset\n",
    "counts = {}\n",
    "\n",
    "for l in LABELS:\n",
    "    counts[l] = len(os.listdir(os.path.join(DATASET, l)))\n",
    "    \n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar(range(len(counts)), list(counts.values()))\n",
    "plt.xticks(range(len(counts)), list(counts.keys()), rotation=40)\n",
    "plt.xlabel('class label')\n",
    "plt.ylabel('class size')\n",
    "plt.title('EUROSAT Class Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review a few samples of the training/validation dataset\n",
    "img_paths = [os.path.join(DATASET, l, l+'_1.jpg') for l in LABELS]\n",
    "\n",
    "img_paths = img_paths + [os.path.join(DATASET, l, l+'_2.jpg') for l in LABELS]\n",
    "\n",
    "def plot_sat_imgs(paths):\n",
    "    plt.figure(figsize=(3, 15))\n",
    "    for i in range(20):\n",
    "        plt.subplot(10, 2, i+1, xticks=[], yticks=[])\n",
    "        img = PIL.Image.open(paths[i], 'r')\n",
    "        plt.imshow(np.asarray(img))\n",
    "        #plt.title(paths[i].split('/')[-2])\n",
    "\n",
    "plot_sat_imgs(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewing the path for the data for future pulling\n",
    "img_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewing the called datapoint\n",
    "img1 = Image.open(img_paths[0])\n",
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'image size: {img1.size}')\n",
    "print(f'first pixel in RGB values: {img1.getpixel((0, 0))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm shape of the data\n",
    "input1 = np.array(img1)\n",
    "flattened_input1 = np.ravel(input1)/255 #normalized array\n",
    "input1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Data to feed into Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for loading images from the directory\n",
    "target_size = (64, 64)  # Resize images to a consistent size, if not already done so\n",
    "batch_size = 32  # Number of images to load at each iteration\n",
    "class_mode = 'categorical'  # Use 'binary' for binary classification, 'categorical' for multi-class classification\n",
    "\n",
    "# Create an ImageDataGenerator instance\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to be between 0 and 1\n",
    "    validation_split=0.2,  #split data into training and validation sets\n",
    "    rotation_range=360 #randomly rotate the images across all possible values (0-360) - will help with overfit models\n",
    "    )\n",
    "\n",
    "# Load training data\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    DATASET,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode,\n",
    "    subset='training',  # Specify 'training' to load the training set\n",
    "    seed=53 #setting seed for re-producability\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    DATASET,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode,\n",
    "    subset='validation',  # Specify 'validation' to load the validation set\n",
    "    seed=53, #setting seed for re-producability\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "#create an array of the classes for future use\n",
    "true_classes = []\n",
    "for i in range(len(validation_generator)):\n",
    "    images, labels = validation_generator[i]\n",
    "    batch_labels = np.argmax(labels, axis=1)\n",
    "    true_classes.append(batch_labels)\n",
    "\n",
    "flat_list = [item for sublist in true_classes for item in sublist]\n",
    "true_classes = flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review the classes\n",
    "class_indices = train_generator.class_indices\n",
    "class_names = {v: k for k, v in class_indices.items()}\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given time restraints, the entire dataset can take up to 4 hours to run through a model; therefore, will create a \"sample\" dataset that is a fraction of the entire train/validation dataset, determine the best model, then fit that model to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator instance for a sample-sized dataset - for faster-running base models\n",
    "sample_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to be between 0 and 1\n",
    "    validation_split=0.2, #split data into training and validation sets\n",
    "    rotation_range=360\n",
    "    )\n",
    "\n",
    "target_size = (64, 64)  # Resize images to a consistent size\n",
    "batch_size = 20  # Number of images to load at each iteration\n",
    "class_mode = 'categorical'  # Use 'binary' for binary classification, 'categorical' for multi-class classification\n",
    "\n",
    "# Load sample data into train\n",
    "sample_train_generator = sample_datagen.flow_from_directory(\n",
    "    'sample_img',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode,\n",
    "    subset='training',\n",
    "    seed=53\n",
    ")\n",
    "\n",
    "#load sample data into validation\n",
    "sample_val_generator = sample_datagen.flow_from_directory(\n",
    "    'sample_img',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode,\n",
    "    subset='validation',\n",
    "    seed=53,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "#create an array of the classes for the validation data\n",
    "true_classes_sample = []\n",
    "for i in range(len(sample_val_generator)):\n",
    "    images, labels = sample_val_generator[i]\n",
    "    batch_labels = np.argmax(labels, axis=1)\n",
    "    true_classes_sample.append(batch_labels)\n",
    "\n",
    "flat_list_sample = [item for sublist in true_classes_sample for item in sublist]\n",
    "true_classes_sample = flat_list_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up early stopping, as many of the models converge fairly quickly\n",
    "early_stopping = EarlyStopping(monitor='accuracy',  \n",
    "                               patience=20,          # number of epochs with no improvement after which training will be stopped\n",
    "                               verbose=1,           # logs messages\n",
    "                               restore_best_weights=True)  # restore model weights from the epoch with the best value of the monitored quantity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying a simple flat-model, for reference\n",
    "model0 = Sequential()\n",
    "\n",
    "#adding in a flatten activation\n",
    "model0.add(Flatten(input_shape=(64, 64, 3)))\n",
    "\n",
    "model0.add(Dense(64, activation='relu', kernel_regularizer = l2(3e-3)))\n",
    "model0.add(Dense(64, activation='relu', kernel_regularizer = l2(3e-3)))\n",
    "model0.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model0.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnn0 = model0.fit(sample_train_generator, validation_data=sample_val_generator, \n",
    "                         epochs=200, batch_size=32, verbose = 2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review the accuracy metrics by epoch\n",
    "sample_cnn_history0 = pd.DataFrame(sample_cnn0.history)\n",
    "sample_cnn_history0.index.name = 'epochs'\n",
    "\n",
    "col_list = ['accuracy', 'val_accuracy']\n",
    "sample_cnn_history0[col_list].plot()\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Training accuracy history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat model understandably isn't very accurate, moving on to a baseline CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample = Sequential()\n",
    "# define 3x3 filter window sizes. Create 32 filters.\n",
    "# COv2D input shape =(image_height, image_width, color_channels) for each image\n",
    "model_sample.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(64, 64, 3)))\n",
    "# max pool in 2x2 window\n",
    "model_sample.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "# transition to dense fully-connected part of network\n",
    "model_sample.add(Flatten())\n",
    "model_sample.add(Dense(64, activation='relu'))\n",
    "model_sample.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnn = model_sample.fit(sample_train_generator, validation_data=sample_val_generator, \n",
    "                              epochs=200, batch_size=32, verbose = 2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnn_history = pd.DataFrame(sample_cnn.history)\n",
    "sample_cnn_history.index.name = 'epochs'\n",
    "\n",
    "col_list = ['accuracy', 'val_accuracy']\n",
    "sample_cnn_history[col_list].plot()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training accuracy history - Model 1 sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model runs much better than the flat Neural Network - try new regularized model on sample data before feeding in entire dataset\n",
    "\n",
    "In theory, regularization should help reduce the overfitting going on by avoiding the weights from exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "# define 3x3 filter window sizes. Create 32 filters.\n",
    "# COv2D input shape =(image_height, image_width, color_channels) for each image\n",
    "model2.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(64, 64, 3),\n",
    "                        kernel_regularizer = l2(3e-3) ))\n",
    "# max pool in 2x2 window\n",
    "model2.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "\n",
    "model2.add(Conv2D(filters=32,\n",
    "                        kernel_size=(4, 4),\n",
    "                        activation='relu',\n",
    "                        kernel_regularizer = l2(3e-3) ))\n",
    "\n",
    "# transition to dense fully-connected part of network\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(64, activation='relu', kernel_regularizer = l2(3e-3)))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnn2 = model2.fit(sample_train_generator, validation_data=sample_val_generator, \n",
    "                         epochs=100, batch_size=32, verbose = 2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnn_history2 = pd.DataFrame(sample_cnn2.history)\n",
    "sample_cnn_history2.index.name = 'epochs'\n",
    "best_epoch = sample_cnn2.epoch[-1]\n",
    "print(best_epoch)\n",
    "col_list = ['accuracy', 'val_accuracy']\n",
    "sample_cnn_history2[col_list].plot()\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Training accuracy history - model 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This runs slightly better than the original base model; add Dropout filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addingdropout layers\n",
    "model3 = Sequential()\n",
    "\n",
    "# define 3x3 filter window sizes. Create 32 filters.\n",
    "# COv2D input shape =(image_height, image_width, color_channels) for each image\n",
    "model3.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(64, 64, 3),\n",
    "                        kernel_regularizer = l2(3e-3) ))\n",
    "# max pool in 2x2 window\n",
    "model3.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(Conv2D(filters=32,\n",
    "                        kernel_size=(4, 4),\n",
    "                        activation='relu',\n",
    "                        kernel_regularizer = l2(3e-3) ))\n",
    "model3.add(Dropout(0.25))\n",
    "# transition to dense fully-connected part of network\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(64, activation='relu', kernel_regularizer = l2(3e-3)))\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnn3 = model3.fit(sample_train_generator, validation_data=sample_val_generator, \n",
    "                         epochs=100, batch_size=32, verbose = 2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnn_history3 = pd.DataFrame(sample_cnn3.history)\n",
    "sample_cnn_history3.index.name = 'epochs'\n",
    "col_list = ['accuracy', 'val_accuracy']\n",
    "sample_cnn_history3[col_list].plot()\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Training accuracy history - model 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 - adding more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addingdropout layers\n",
    "model4 = Sequential()\n",
    "\n",
    "# define 3x3 filter window sizes. Create 32 filters.\n",
    "# COv2D input shape =(image_height, image_width, color_channels) for each image\n",
    "model4.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(64, 64, 3),\n",
    "                        kernel_regularizer = l2(3e-3) ))\n",
    "# max pool in 2x2 window\n",
    "model4.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "\n",
    "model4.add(Dropout(0.25))\n",
    "model4.add(Conv2D(filters=32,\n",
    "                        kernel_size=(4, 4),\n",
    "                        activation='relu',\n",
    "                        kernel_regularizer = l2(3e-3) ))\n",
    "model4.add(Conv2D(filters=16,\n",
    "                        kernel_size=(4, 4),\n",
    "                        activation='relu',\n",
    "                        kernel_regularizer = l2(3e-3) ))\n",
    "model4.add(Dropout(0.25))\n",
    "model4.add(Conv2D(filters=16,\n",
    "                        kernel_size=(2, 2),\n",
    "                        activation='relu',\n",
    "                        kernel_regularizer = l2(3e-3) ))\n",
    "\n",
    "# transition to dense fully-connected part of network\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(64, activation='relu', kernel_regularizer = l2(3e-3)))\n",
    "model4.add(Dense(32, activation='relu'))\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model4.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnn4 = model4.fit(sample_train_generator, validation_data=sample_val_generator, \n",
    "                         epochs=100, batch_size=32, verbose = 2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cnn_history4 = pd.DataFrame(sample_cnn4.history)\n",
    "sample_cnn_history4.index.name = 'epochs'\n",
    "col_list = ['accuracy', 'val_accuracy']\n",
    "sample_cnn_history4[col_list].plot()\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Training accuracy history - model 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the accuracy graphs for each model, in order to determine which model to run Cross-validation on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all the accuracy results on one cell\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n",
    "\n",
    "col_list = ['accuracy', 'val_accuracy']\n",
    "\n",
    "sns.lineplot(data=sample_cnn_history[col_list], ax=axs[0,0])\n",
    "sns.lineplot(data=sample_cnn_history2[col_list], ax=axs[0,1])\n",
    "sns.lineplot(data=sample_cnn_history3[col_list], ax=axs[1,0])\n",
    "sns.lineplot(data=sample_cnn_history4[col_list], ax=axs[1,1])\n",
    "\n",
    "axs[0,0].set_title('Model 1 - baseline')\n",
    "axs[0,1].set_title('Model 2 - regularized')\n",
    "axs[1,0].set_title('Model 3 - regularized with dropout')\n",
    "axs[1,1].set_title(\"Model 4 - regularized, dropouts, add'l layers\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given the above performances, will use the model with the best train/validation accuracy scores, and see if training on the entire dataset results in higher accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model 1:'+str(sample_cnn_history[col_list].max()))\n",
    "print('model 2:'+str(sample_cnn_history2[col_list].max()))\n",
    "print('model 3:'+str(sample_cnn_history3[col_list].max()))\n",
    "print('model 4:'+str(sample_cnn_history4[col_list].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(saving all models!)\n",
    "model0.save('model0.h5')\n",
    "model_sample.save('model1.h5')\n",
    "model2.save('model2.h5')\n",
    "model3.save('model3.h5')\n",
    "model4.save('model4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned model ran on full labeled dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note - the full model fit will take ~3 hours; loaded the sample-trained model, fitting, and re-saving the model\n",
    "loaded_model = load_model('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_cnn = loaded_model.fit(train_generator, \n",
    "                              epochs=200, batch_size=32, verbose = 2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.save('loaded_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_history1 = pd.DataFrame(full_model_cnn.history)\n",
    "cnn_history1.index.name = 'epochs'\n",
    "\n",
    "col_list = ['accuracy']\n",
    "cnn_history1[col_list].plot()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training accuracy history - Full Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeated the loading, as the notebook was shut down after training\n",
    "loaded_model = load_model('loaded_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on the unseen validation data\n",
    "predictions_full = loaded_model.predict(validation_generator)\n",
    "#class labels\n",
    "predicted_labels_full = np.argmax(predictions_full, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review the confusion matrix for the full model\n",
    "confusion_full = confusion_matrix(true_classes, predicted_labels_full)\n",
    "disp4 = ConfusionMatrixDisplay(confusion_matrix=confusion_full)\n",
    "disp4.plot()\n",
    "legend_text = '\\n'.join([f'{key}: {value}' for key, value in class_names.items()])\n",
    "plt.text(15, 6, legend_text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(true_classes, predicted_labels_full)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "recall = recall_score(true_classes, predicted_labels_full, average='weighted')\n",
    "print(\"Recall:\", recall)\n",
    "f1 = f1_score(true_classes, predicted_labels_full, average='weighted')\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the full dataset, we received slightly higher accuracy scores at 82%. I may use this model in the future and do some hyper-parameter tuning, when time and resources are available. some notes on future steps:\n",
    "\n",
    "1. ReLu appears to be the best activation methodology for neural network layers, however it would be interesting to try other available methodologies (tanh, linear, sigmoid etc.)\n",
    "2. the filters were largely set at 32, however it would be interesting to evaluate different metrics here\n",
    "3. ridge regularizers were exclusively used, yet I would like to evaluate lasso, as well as different strengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL MODEL RUN ON UNSEEN DATASET\n",
    "\n",
    "Using my domain knowledge (born and raised in this area!), I would like to take satellite imagery from the Springfield, MA metropolitan area and see how well the model can classify sections of the image. (taken from Google Earth, 25 October 2023).\n",
    "\n",
    "Steps are as follows:\n",
    "\n",
    "1. load in the image\n",
    "1. create \"chunks\" of the image - given that the model was trained on 64x64 pixel images, i will \"chunk\" the satellite image into these proportions. NOTE: for future improvement, I will rescale based on any given image size. However, I ensured the satellite image I captured had a width & height that is divisible by 64, to ensure the chunks are evenly spaced for future plotting\n",
    "1. save the chunks, feed into an array that will fit into the trained full model\n",
    "1. predict the classifications on each of the chunks\n",
    "1. Plot the chunk classifications back onto the original Satellite image\n",
    "- this ended up becoming more manual than I expected - notes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test image of Springfield, MA metro area subset\n",
    "test_image = Image.open('SPFLD_test.jpg')\n",
    "test_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the size\n",
    "test_image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the breakout of the test image in folder `test_img` - only need to run this code once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chunk size (square shape, number equaling each pixel width/height)\n",
    "chunk_size = 64\n",
    "\n",
    "# Get the dimensions of the original image\n",
    "width, height = test_image.size\n",
    "\n",
    "# Iterate through the original image in 64x64 chunks\n",
    "for i in range(0, height, chunk_size):\n",
    "    for j in range(0, width, chunk_size):\n",
    "        # Define the region to crop\n",
    "        left = j\n",
    "        upper = i\n",
    "        right = j + chunk_size\n",
    "        lower = i + chunk_size\n",
    "\n",
    "        # Crop the chunk from the original image\n",
    "        chunk = test_image.crop((left, upper, right, lower))\n",
    "\n",
    "        # Save the chunk as a new image - the coordinates in the naming convention are crucial for future plotting!\n",
    "        chunk.save(f\"test_img/chunk_{i}_{j}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk_files = os.listdir(\"test_img\")\n",
    "\n",
    "# Initialize an empty array to store the chunks\n",
    "chunks_data = []\n",
    "chunk_file = []\n",
    "width, height = test_image.size\n",
    "chunk_size = 64\n",
    "\n",
    "for i in range(0, height, chunk_size):\n",
    "    for j in range(0, width, chunk_size):\n",
    "        # Define the region to crop\n",
    "        left = j\n",
    "        upper = i\n",
    "        right = j + chunk_size\n",
    "        lower = i + chunk_size\n",
    "    # Load each chunk and preprocess it - this is where the naming convention ensures it is loaded in the correct order!\n",
    "        chunk_file.append(f\"test_img/chunk_{i}_{j}.jpg\")\n",
    "        chunk = Image.open(os.path.join(f\"test_img/chunk_{i}_{j}.jpg\")).resize((64, 64))\n",
    "        chunk_array = np.array(chunk) / 255.0  # Normalize the pixel values between 0 and 1\n",
    "        chunks_data.append(chunk_array)\n",
    "\n",
    "# Convert the list of chunks into a NumPy array\n",
    "test_data = np.array(chunks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure the shape is correct, especially in dimensions 2-4\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that chunk file #1 is chunk_0_64\n",
    "chunk_file[]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save test_data to a .npz file - ran once. Noted that the saved array is massive in storage size, so will save this code but not run again\n",
    "np.savez(\"prov_data.npz\", test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#un-hash the below to upload the saved array - for future work\n",
    "# Access the test_data array from the loaded file\n",
    "#test_data = loaded_data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify each image chunk using the loaded model from above\n",
    "test_preds = loaded_model.predict(test_data)\n",
    "predicted_classes_test = np.argmax(test_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure the shape is still accurate\n",
    "predicted_classes_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brief EDA on the predictions\n",
    "# Get the unique values and their counts\n",
    "unique_values, counts = np.unique(predicted_classes_test, return_counts=True)\n",
    "\n",
    "# Print the unique values and their counts\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"Class: {value}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reminder of the class names for each value 0-9:\n",
    "list(class_names.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA on one given class, Industrial:\n",
    "display = 4    \n",
    "display_vals = [index for index, value in enumerate(predicted_classes_test) if value == display][:15]\n",
    "display_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show images that were labeled as class defined under variable \"display\" above - in this case, Industrial:\n",
    "display = 4    \n",
    "display_vals = [index for index, value in enumerate(predicted_classes_test) if value == display][:15]\n",
    "    \n",
    "def plot_test_imgs(paths):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(15):\n",
    "        plt.subplot(3, 5, i+1, xticks=[], yticks=[])\n",
    "        img = Image.open(os.path.join(chunk_file[display_vals[i]]))\n",
    "        plt.imshow(np.asarray(img))\n",
    "\n",
    "plot_test_imgs(display_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving on to plotting the chunks back onto the original shape with their class.\n",
    "\n",
    "Note: a bit manual, in the future I plan to automate the below sections for ease of visualization.\n",
    "\n",
    "1. Create a NumPy array ot zeroes that matches the size of the original image in pixels\n",
    "1. iterate through the chunks (IN THE SAME ORDER THAT THE CHUNKS ARE STORED IN THE INDEX) and assign the predicted class to the pixels that the chunk correlates to in the original image\n",
    "1. use .imshow (https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) to plot each class over the original image. for visibility, I played around with the \"alpha\" (transparency) and landed largely on 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty array to store the predicted classes mapped to the original image shape\n",
    "mapped_predictions = np.zeros((4224, 8192))\n",
    "chunk_size = 64\n",
    "\n",
    "# Iterate through the chunks and map predictions back to the original image\n",
    "index = 0\n",
    "for i in range(0, 4224, chunk_size):\n",
    "    for j in range(0, 8192, chunk_size):\n",
    "        # Get the predicted class for the current chunk\n",
    "        predicted_class = predicted_classes_test[index]\n",
    "\n",
    "        # Map the predicted class to the corresponding region in the original image\n",
    "        mapped_predictions[i:i+chunk_size, j:j+chunk_size] = predicted_class\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once again, confirm the size. NOTE: for array, width and height are flipped. this makes sense, \n",
    "# since we are looking at an array in the notation of (rows, columns) instead of (width, height)\n",
    "test_image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirming inverse of the image size is the numpy array shape\n",
    "mapped_predictions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 2, figsize=(10, 10))\n",
    "\n",
    "#clear xticks - we just want to see the image for now\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plotindex=0\n",
    "\n",
    "#loop through each class and make a separate plot for each\n",
    "for i in range (5):\n",
    "    for j in range(2):\n",
    "        if plotindex < len(class_names):\n",
    "            # Plot the original image\n",
    "            axs[i,j].imshow(test_image)\n",
    "            #plot the predicted class by index\n",
    "            mask = (mapped_predictions == plotindex)\n",
    "            axs[i,j].imshow(mask, alpha=0.5)\n",
    "            axs[i,j].set_title(list(class_names.values())[plotindex])\n",
    "            axs[i,j].set_xticks([])\n",
    "            axs[i,j].set_yticks([])\n",
    "        else:\n",
    "            axs[i,j].axis('off') \n",
    "        plotindex += 1\n",
    "\n",
    "plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing a few classes at larger scale - again, starting with industrial\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "# Plot the original image\n",
    "ax.imshow(test_image)\n",
    "#plot the class - 'Industrial'\n",
    "mask = (mapped_predictions == 4)\n",
    "ax.imshow(mask, alpha=0.5)#, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the same for \"Herbaceous vegetation\"\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "# Plot the original image\n",
    "ax.imshow(test_image)\n",
    "#plot the class - 'LakeSea'\n",
    "mask = (mapped_predictions == 2)\n",
    "ax.imshow(mask, alpha=0.5)#, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#attempt looking at all the classes on the same map by color gradient\n",
    "# Define a colormap with a unique color for each class\n",
    "num_classes = len(np.unique(mapped_predictions))\n",
    "colors = plt.cm.get_cmap('tab20', num_classes) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Plot the original image\n",
    "ax.imshow(test_image)\n",
    "ax.imshow(mapped_predictions, cmap='viridis', interpolation='nearest', alpha=0.4)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - I wasn't sure where this would go, but it doesn't appear to be giving much value when combining all. Keeping for future use, as this may prove useful in other test images.\n",
    "\n",
    "One last visualization for curiosity - looking at the graph of each class mapped out, it appears that the water features are classified as river, Sea/Lake, and even forest (note: it appears this is due to the \"forest\" training data having a very dark green/blue hue). How would it look if all these classifications were wrapped into one image/class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just for fun\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "# Plot the original image\n",
    "ax.imshow(test_image)\n",
    "#plot the class - 'LakeSea', 'forest', or 'river'\n",
    "mask = np.isin(mapped_predictions, [1, 8, 9])\n",
    "ax.imshow(mask, alpha=0.5)#, cmap='coolwarm')\n",
    "ax.set_title('combined forest, river, sea/lake classifications')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's actually much more accurate! There is potential here for \"class refinement\" for future models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#One last \"just for fun\" - see if it can predict where the people live.\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "# Plot the original image\n",
    "ax.imshow(test_image)\n",
    "#plot the class - 'LakeSea', 'forest', or 'river'\n",
    "mask = np.isin(mapped_predictions, [7])\n",
    "ax.imshow(mask, alpha=0.5)#, cmap='coolwarm')\n",
    "ax.set_title('Residential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Overall, the model performed well at 82% accuracy. As mentioned throughout, there are multiple paths to go in order to improve the model, though arguably the best factor would be incremental data on a like-for-like satellite imagery. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
